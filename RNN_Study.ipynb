{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9205985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331118a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b19b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(_status.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19c2dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True, bidirectional=True)\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(_status.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dae791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True, bidirectional=True)\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(_status[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3dc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d09b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1225b666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', 'a', 'e', 'l', 'p']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bd50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb93bc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
    "print(char_to_index)\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df9bf7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n",
      "[[[0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "x_one_hot = np.array([np.eye(vocab_size)[x] for x in x_data])\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "155db95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "X = torch.FloatTensor(x_one_hot).to(device)\n",
    "Y = torch.LongTensor(y_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "667169a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "\n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d257374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "               RNN-1   [[-1, 1, 5], [-1, 2, 5]]               0\n",
      "            Linear-2                 [-1, 1, 5]              30\n",
      "================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "\n",
    "\n",
    "net = Net(input_size, hidden_size, output_size).to(device)\n",
    "torchsummary.summary(net, input_size=(1, input_size), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0323b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.view(-1, input_size).shape)\n",
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebe660a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "256dd811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.1513410806655884 prediction:  tensor([[4, 4, 3, 4, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplp!\n",
      "1 loss:  0.9466041326522827 prediction:  tensor([[4, 4, 3, 4, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplp!\n",
      "2 loss:  0.7849953770637512 prediction:  tensor([[4, 4, 3, 4, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplp!\n",
      "3 loss:  0.6621822118759155 prediction:  tensor([[4, 4, 3, 4, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplp!\n",
      "4 loss:  0.5500205159187317 prediction:  tensor([[4, 4, 3, 4, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplp!\n",
      "5 loss:  0.45314115285873413 prediction:  tensor([[4, 4, 3, 4, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplp!\n",
      "6 loss:  0.3795475959777832 prediction:  tensor([[4, 4, 3, 4, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplp!\n",
      "7 loss:  0.31413987278938293 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.3077954649925232 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.22019024193286896 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.19943620264530182 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.15638592839241028 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.10787466913461685 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.07587509602308273 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.055121950805187225 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.03906082361936569 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.027062341570854187 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.019303197041153908 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.014594560489058495 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.011563637293875217 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.00941817183047533 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.0077969892881810665 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.006531215272843838 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.0055284518748521805 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.004727744963020086 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.004084615968167782 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.0035650269128382206 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.0031424276530742645 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.002796328393742442 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0025106631219387054 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.002272965619340539 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0020738448947668076 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.001905691111460328 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.0017625667387619615 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.0016399268060922623 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.0015342173865064979 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.001442475477233529 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.0013623045524582267 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.001291925786063075 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0012297718785703182 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0011746558593586087 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.0011255086865276098 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.00108149868901819 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0010419131722301245 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0010061337379738688 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0009737091022543609 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0009441872825846076 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.0009173069265671074 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0008925682632252574 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0008698524907231331 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.0008489456959068775 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0008296096930280328 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.0008117019897326827 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.0007950320723466575 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.000779361987952143 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.0007648823084309697 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0007511880830861628 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0007383033516816795 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.0007262519793584943 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0007148195873014629 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0007040539057925344 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0006938120350241661 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0006840702262707055 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0006747808074578643 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.0006659675855189562 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.000657511583995074 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0006493651308119297 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0006416711257770658 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0006341914413496852 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0006270214216783643 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0006200893549248576 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.0006133716669864953 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0006069398368708789 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.0006007222691550851 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.0005946476012468338 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0005887873121537268 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0005830221925862134 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0005774953169748187 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 loss:  0.0005720874178223312 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0005668463418260217 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0005617003771476448 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0005567451007664204 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0005518612451851368 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0005470725591294467 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.0005424030241556466 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.0005378049099817872 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0005333496956154704 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0005289421533234417 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0005246297805570066 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0005204127519391477 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0005163386813364923 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.0005122169386595488 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0005082142306491733 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.000504306866787374 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.0005004232516512275 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.0004966587293893099 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.0004928942071273923 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0004892726428806782 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.00048562721349298954 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.00048202945617958903 prediction:  tensor([[4, 4, 3, 2, 0]], device='cuda:0') true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "    result = outputs.data.argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c.item()] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b692478",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032811f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0, 'n': 1, 'r': 2, 's': 3, ',': 4, 'i': 5, 'a': 6, 'w': 7, '.': 8, 'p': 9, 't': 10, 'h': 11, \"'\": 12, 'y': 13, 'c': 14, 'd': 15, 'm': 16, 'k': 17, 'f': 18, 'g': 19, ' ': 20, 'o': 21, 'u': 22, 'b': 23, 'l': 24}\n"
     ]
    }
   ],
   "source": [
    "char_set = list(set(sentence))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}\n",
    "print(char_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea05222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = len(char_dic)\n",
    "sequence_length = 10\n",
    "LR = 0.1\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i+sequence_length]\n",
    "    y_str = sentence[i+1:i+sequence_length+1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a04d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers=layers, batch_first=True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, hidden_size, bias=True) # 출력층 구현\n",
    "\n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb9e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(len(char_dic), hidden_size, 2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74759c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n",
      "torch.Size([170, 10])\n",
      "torch.Size([170, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "x_one_hot = np.array([np.eye(len(char_dic))[x] for x in x_data]) # x 데이터는 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "outputs = net(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23874f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1700, 25])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, len(char_dic)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f7a0162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfbbbbbbgbbbbbbbbgbbbbbbbggbbbbbgbbbbghbbgbbbbbbbbbbbbtbbbbbbbbbbbbbbbbbbbbgbgbgbbgbbbbbgbbbbbbbgbbbbbbgbgbbbbbbbbbbbbgbbbbbbbgbbbbbbbgtbbbbbbbbbbbtbbbbggbbbbbggggbgbbbbbbbtbbbbbg\n",
      "tddd dd dd doddddddddddd d  dd dtodd d dt d  d oddodd dd dtoddoddttddd  ddttd dd dtodtddd ddod ddodd dddd d  dodddd d ddd dtoddt dtd ddoddt d d  dtod dtd d ddddddddddddd dtod dd d\n",
      "        t                                                                                                       t                                                                  \n",
      "ionffwf'nwos,osoon.dwos.o oswonw.oioohot.dl.dw.sw.ooofs.fswo ioswfs,o oolofohhoofnwos.shfsifhofoioshfs.osofsffwosotowfs,osl.oo.o oosootosifswosoos.o gsio fslfs.o ofdfsooow.os.dhfs\n",
      "o tototoshttotl iototoiosl  o tttltttt iottttothltl ttlltlttlot ltlo ottottoiottthhhtt istototlttttototth tt th ttt totl tstotottthtt lt ot lttttlthittt lothtl  lo ltt tt ohiototh\n",
      "t tht tht h h   h t t t t t t   ch   h uh  h t h  h   h    h  h t t   o     h     thah h    h    h h          h u t   tht t h  h tht   h  h   th  ch   hl h       t         h   hs \n",
      "t th cro t    co  t t t t e to ct ch t eo c  the co co cc th co rh   eo c  pl    ch ucerca  huc th ch co  to ct tpe t  ce ut  co rh t eh co c t  cth ct l e t to ct' ch u  c  rt  c\n",
      "t eo        o b   e t e     to to bo t eo   a    to rn    to to te o eo    oe t  'o eo re  to r to    bo  eo bt e e   toe  to to r  o t  to   to  toeoa e e   to     kh ro to  eo '\n",
      "toe dheo toeo t   e toeoe e t eet doat eo r   ee bo rtoe  to to to o do t toe d ett doe'd etoe  th te aoe eo tt eoe dotoe  to te  oer th te   ton eoe toe e t eoe eo kh do to  aod'\n",
      "bhtod.t  toth the d dotoe   dondt doat tr th the th ttoe  dh kh tenh bh t   t dotdt dt.b   the  thlkh dos do k.ot e   the  th kh thet te kh t tonk.oertod d d doe do kh kh th  dod.\n",
      "bhtndfth t to tei d t toe   t  't dost ip ih tee to tther to th teth tn t att t ndt dthyir th   th th dts do kh t t a the  th th ihet th th d tonktoettodne t ioe ts dh ih to tdod.\n",
      "bdtn ftm t to t t m t toep  t  lthdo t tp a  t e to ath m to to tett to t tns tondtodtsigh toem tn ts itd to tt i t a ther to th th r tn th d ton toe and t t in  ts ih in tn mt dt\n",
      "bdtnm tont to tni m tndhipd t slt dost ao a rtse tog ther to to tett to t ans tonlt dtsig  ther to gs dns to g, a t a thermthsct ther to te g ton the ans t t an  ts gh tn whema ts\n",
      "bdtnmlto thtost ig, tothik, tonlt iost to m  tle thslthir to to tsct to c tns t nst ansigo them tos', aoslto g, aue a thir tosch them to ch g tonlthemuos t weio  tsich th themt ts\n",
      "cdto  aout io tme e todhepg tonlt uort ao m rule to  ther to to dero to l and t rlt unsigo ther to le mnslto l, aue ruther thschether to ch d tor themuosle d au  tsigh th theruoa'\n",
      "cdtu eaoue wo tui , t dhip, tonlt uout ao rerule to  ther to wo eeco wonl and tonlt unsigo them tos't ans ton , aut ruther tha'oethem th wo g ton themuoslesd iu etsigy to theruoa'\n",
      "cdtoolgost wo tut d todhip, tonst dunt to psoule to  ther to to eeco wonl tnd ton't dnseg,stoem to ts ind tonl, wut ruther tonco them to to g ton themuoslesd iu etsity to thersot'\n",
      "cdlooigent wo b i , tndhip, ton't dout ao peoc ento  cher to co dect wonl and ton't dnsigo them toskh ind tonk, wut rether to ch them to ghng ton thersnd est immet'igy th therd ts\n",
      "cdeoo go t to t ig, tndhip, ton't dout ao aeop ento  ther to to dect tonk tnd ton't dnsigo them tosks and tonk, but rether to ch them to tong ton thererd est im et'sty tn therd ts\n",
      "cdeoo arnt to buile t dhep, ton't aout ap aeop e thg nher to bo lect tonk and ton't ansigeether tosks and donk, but rather to ch ther to bong ton th rend esd ammetsity an therdfds\n",
      "pdeao aaat to tui d t dhip, toc't dpam ap aeople thg ther to to lect tonk tnd don't dnsign them tosks and donkt but rather tonch them to bong tor therend esd immenslty an thershds\n",
      "pdeao aant to build t dhip, ton't daat ap aeople thg ther to bo lect tool and don't dnsign ther tosks and dook, but rather toach the  to bo g tor therend esshbmaeasity as the dods\n",
      "cdeoo gant to build t dhip, ton't aaum ap aeople to ether to bo lect wook and don't ansign ther tosks and dook, tut rather toach ther to bo g tor thereodle s immensity as therdeap\n",
      "caeoo gant to tuild a ship, don't drum ip people together to bo lect wonk and don't dnsign them tosts and donk, but rather toach them to lo g tor the endless immensity as the aeas\n",
      "caeou gant to cuild d ship, don't drum ip people together to co lect wonk and won't dnsign them tosks and donk, but rather tonch them to long tor the andless immensity os the aeas\n",
      "caeou gant to tuild d whip, don't drum ip people together to co lect wonk and won't dnsign them tosks and donk, but ranher tonch them to long tor the endless immensity of the eeus\n",
      "uaeou gant to build d whip, don't drum pp people together to coldect word and won't dssign them tosks and dork, but rather toach them to lond tor the endless immensity of the aems\n",
      "uaeou aant to cuild d ship, don't drum pp people together to coleect word and don't dssign the  tosks and dork, but rather toach them to lorg tor the andless immensity of the aeas\n",
      "uaeou aant to build a ship, don't arum up people together to coleect word and don't assign them tosks and work, but rather toach them to cong tor the endless immensity of the aogc\n",
      "uaeom want to cuild a ship, don't arum up people together to collect word and don't assign them tosks and dork, but rather tonch them to long tor the endless immensity of the aeal\n",
      "uafou want to build a ship, don't drum up people together to collect word and don't assign them tosks and dork, but rather tonch them to long tor the endless immensity of the eeal\n",
      "uafou want to build a ship, don't drum up people together to co lect word and don't dssign them tosks and dork, but rather toach them to long tor the endless immensity of the ehsl\n",
      "uafou want to build a ship, don't drum up people together to lollect word and don't dssign them tosks and work, but rather toach them to long tor the endless immensity of the ehnp\n",
      "pafouewant to build a ship, don't drum up people together to collect word and don't dssign them tosks and work, but rather toach them to long tor the endless immensity of the eeny\n",
      "pafouewant to build a ship, don't drum up people together to lollect word and don't dssign them tasks and work, but rather toach them to long tor the endless immensity of the eea.\n",
      "tayouiwant to build a ship, don't drum up people together to collect word and don't dssign them tasks and work, but rather toach them to long tor the endless immensity of the eea.\n",
      "tayouiwant to build a ship, don't drum up people together to collect word and don't dssign them tasks and dork, but rather toach them to long tor the endless immensity of the eea.\n",
      "tayou want to build a ship, don't drum up people together to collect word and don't dssign them tasks and work, but rather toach them to long tor the endless immensity of the eea.\n",
      "tayou want to build a ship, don't arum up people together to collect word and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "tayou want to build a ship, don't arum up people together to collect word and won't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "tayou want to build a ship, don't drum up people together to collect word and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "tayou want to build a ship, don't drum up people together to collect word and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "tayou want to build a ship, don't drum up people together to collect word and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "tayou want to build a ship, don't drum up people together to collect word and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "tayou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "tayou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "tayou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "tmyou want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "tmyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "tmyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "teyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
      "teyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "teyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "teyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "teyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
    "    loss = criterion(outputs.view(-1, len(char_dic)), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 텐서 크기는 (170, 10)\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ff02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3347f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_test.txt', <http.client.HTTPMessage at 0x1654a7490>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b93c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰 개수 : 150000\n",
      "테스트용 리뷰 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
    "print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0c5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'].nunique(), train_data['label'].nunique()\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e2b688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['label'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b804eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  count\n",
      "0      0  73342\n",
      "1      1  72841\n"
     ]
    }
   ],
   "source": [
    "print(train_data.groupby('label').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d490a10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/r3j93pws41712_dl1qyxtv7w0000gn/T/ipykernel_42771/2092559641.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c74babd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 48852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/r3j93pws41712_dl1qyxtv7w0000gn/T/ipykernel_42771/2642594262.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
      "/var/folders/x0/r3j93pws41712_dl1qyxtv7w0000gn/T/ipykernel_42771/2642594262.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
      "/var/folders/x0/r3j93pws41712_dl1qyxtv7w0000gn/T/ipykernel_42771/2642594262.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa350bb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/konlpy/tag/_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger \u001b[38;5;241m=\u001b[39m \u001b[43mTagger\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-d \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/data/tagset/mecab.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m utils\u001b[38;5;241m.\u001b[39minstallpath)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tagger' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m도\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m는\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m의\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m가\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m이\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m은\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m한\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m에\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m하\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m고\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m을\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m를\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m인\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m듯\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m과\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m와\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m네\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m들\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m듯\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m지\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m임\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m게\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m mecab \u001b[38;5;241m=\u001b[39m \u001b[43mMecab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/konlpy/tag/_mecab.py:82\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe MeCab dictionary does not exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Is the dictionary correctly installed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMecab(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m/some/dic/path\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
     ]
    }
   ],
   "source": [
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a418d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
